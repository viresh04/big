nano mapper.py

import sys
for line in sys.stdin:
    line = line.strip()
    # split the line into words
    words = line.split()
    for word in words:
        
        print '%s\t%s' % (word, 1)


nano reducer.py

from operator import itemgetter
import sys
current_word = None
current_count = 0
word = None
	
for line in sys.stdin:
        line = line.strip()

    
    word, count = line.split('\t', 1)

    
    try:
        count = int(count)
    except ValueError:
        # count was not a number, so silently
        # ignore/discard this line
        continue

    # this IF-switch only works because Hadoop sorts map output
    # by key (here: word) before it is passed to the reducer
    if current_word == word:
        current_count += count
    else:
        if current_word:
            # write result to STDOUT
            print '%s\t%s' % (current_word, current_count)
	current_count = count
        current_word = word

# do not forget to output the last word if needed!
if current_word == word:
    print '%s\t%s' % (current_word, current_count)

**************************************************************************
cat word_count_data.txt | python mapper.py  | sort -k1,1 | python reducer.py 
star-all.sh
jps
dfs -mkdir /word_count_in_python
 hdfs dfs -copyFromLocal /home/hduser/Documents/word_count_data.txt  /word_count_in_python


hdfs dfs -ls /

hdfs dfs -ls / word_count_in_python


chmod 777 mapper.py reducer.py 



 hadoop jar /home/hduser/Documents/hadoop-streaming-2.7.3.jar \


> -input /word_count_in_python/word_count_data.txt \
> -output /word_count_in_python/output \
> -mapper /home/hduser/Documents/mapper.py \
> -reducer /home/hduser/Documents/reducer.py 

Output directory: /word_count_in_python/output

hdfs dfs -cat /word_count_in_python/output/part-00000



